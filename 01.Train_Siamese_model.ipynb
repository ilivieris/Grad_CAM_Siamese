{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.load_transformations import load_transformations\n",
    "from utils.dataset import Dataset\n",
    "from utils.utils import get_dataset, create_instances, format_time\n",
    "from utils.Siamese import SiameseNetwork\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.LRScheduler import LRScheduler\n",
    "from utils.performance_evaluation import performance_evaluation\n",
    "\n",
    "# Get configuration\n",
    "with open(\"config.yml\", 'r') as stream:\n",
    "    params = yaml.safe_load(stream)\n",
    "if params['cuda']:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.seed(params['seed'])\n",
    "torch.manual_seed(params['seed'])\n",
    "torch.cuda.manual_seed(params['seed'])\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(params['seed'])\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "df = get_dataset(params)\n",
    "# # Split to train/valid/test\n",
    "df_train, df_test = train_test_split(df, test_size=params['test_size'], random_state=params['seed'])\n",
    "df_train, df_valid = train_test_split(df_train, test_size=params['valid_size'], random_state=params['seed'])\n",
    "# Create positive/negative instances\n",
    "train_dataset = create_instances(df=df_train, \n",
    "                                 number_of_iterations=params['number_of_iterations'])\n",
    "valid_dataset = create_instances(df=df_valid, \n",
    "                                 number_of_iterations=params['number_of_iterations'])\n",
    "test_dataset = create_instances(df=df_test, \n",
    "                                number_of_iterations=params['number_of_iterations'])                                                                  \n",
    "# Get training/testing image transformations\n",
    "train_tfms, test_tfms = load_transformations(params)\n",
    "\n",
    "\n",
    "# Create loaders\n",
    "train_dl = DataLoader(dataset = Dataset(data=train_dataset, tfms=train_tfms),\n",
    "                      batch_size  = params['hyperparameters']['batch_size'],\n",
    "                      shuffle     = True, \n",
    "                      num_workers = params['hyperparameters']['num_workers'], \n",
    "                      pin_memory  = True)\n",
    "\n",
    "\n",
    "valid_dl = DataLoader(dataset = Dataset(data=valid_dataset, tfms=test_tfms),\n",
    "                      batch_size  = params['hyperparameters']['batch_size'],\n",
    "                      shuffle     = False, \n",
    "                      num_workers = params['hyperparameters']['num_workers'], \n",
    "                      pin_memory  = True)\n",
    "\n",
    "test_dl = DataLoader(dataset = Dataset(data=test_dataset, tfms=test_tfms),\n",
    "                      batch_size  = params['hyperparameters']['batch_size'],\n",
    "                      shuffle     = False, \n",
    "                      num_workers = params['hyperparameters']['num_workers'], \n",
    "                      pin_memory  = True)\n",
    "\n",
    "print('[INFO] Training instances: ', train_dataset.__len__())\n",
    "print('[INFO] Validation instances: ', valid_dataset.__len__())\n",
    "print('[INFO] Testing instances: ', test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "model = SiameseNetwork(backbone_model=params['backbone_model']).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "if params['hyperparameters']['optimizer'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=float(params['hyperparameters']['learning_rate']))\n",
    "elif params['hyperparameters']['optimizer'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(params['hyperparameters']['learning_rate']))\n",
    "elif params['hyperparameters']['optimizer'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=float(params['hyperparameters']['learning_rate']))\n",
    "    \n",
    "    \n",
    "scheduler = LRScheduler(optimizer = optimizer, \n",
    "                        patience  = params['LRScheduler']['patience'], \n",
    "                        min_lr    = params['LRScheduler']['min_lr'],\n",
    "                        factor    = params['LRScheduler']['factor'],\n",
    "                        verbose   = params['LRScheduler']['verbose'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience  = params['early_stopping']['patience'],\n",
    "                               min_delta = params['early_stopping']['min_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_AUC = 0.0\n",
    "history = {'train_loss': [], 'valid_loss': [], \n",
    "           'train_accuracy': [], 'valid_accuracy': [], \n",
    "           'train_AUC': [], 'valid_AUC': []}\n",
    "\n",
    "\n",
    "for epoch in range(params['hyperparameters']['epochs']):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Activate training mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(train_dl, leave=True)\n",
    "    # setup epoch's metrics\n",
    "    metrics = {'losses': [], 'accuracy': [], 'AUC': []}\n",
    "    for step, (img1, img2, labels) in enumerate(loop):\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        # initialize calculated gradients \n",
    "        optimizer.zero_grad()\n",
    "        # Get loss and predictions\n",
    "        predictions, loss = model(img1, img2, labels)  \n",
    "        # Calculate performance metrics\n",
    "        accuracy, AUC, _ = performance_evaluation(labels, predictions)        \n",
    "        # Backpropagate errors  \n",
    "        loss.backward()\n",
    "        # Clip gradient norm\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=params['hyperparameters']['max_norm'])\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Add loss\n",
    "        metrics['losses'].append(loss.item())\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['AUC'].append(AUC)        \n",
    "        # add stuff to progress bar in the end\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{params['hyperparameters']['epochs']}]\")\n",
    "        loop.set_postfix(loss=f\"{np.mean(metrics['losses']):.3f}\", \n",
    "                         accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\",\n",
    "                         AUC=f\"{np.mean(metrics['AUC']):.3f}\")\n",
    "        \n",
    "    # Calculate test loss/accuracy/AUC\n",
    "    train_loss = np.mean(metrics['losses'])\n",
    "    train_accuracy = np.mean(metrics['accuracy'])\n",
    "    train_AUC = np.mean(metrics['AUC'])\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    ConfusionMatrix = np.array([[0,0],[0,0]]) # LIVIERIS\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(valid_dl, leave=True)\n",
    "    # setup epoch's metrics\n",
    "    metrics = {'losses': [], 'accuracy': [], 'AUC': []}\n",
    "    for step, (img1, img2, labels) in enumerate(loop):\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        # Get loss & predictions\n",
    "        predictions, loss = model(img1, img2, labels)\n",
    "        # Calculate performance metrics\n",
    "        accuracy, AUC, CM = performance_evaluation(labels, predictions)               \n",
    "        ConfusionMatrix+=CM\n",
    "        # Add loss/accuracy/AUC\n",
    "        metrics['losses'].append(loss.item())\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['AUC'].append(AUC)   \n",
    "\n",
    "    \n",
    "        # add stuff to progress bar in the end\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{params['hyperparameters']['epochs']}]\")\n",
    "        loop.set_postfix(loss=f\"{np.mean(metrics['losses']):.3f}\", \n",
    "                         accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\",\n",
    "                         AUC=f\"{np.mean(metrics['AUC']):.3f}\")\n",
    "    print(ConfusionMatrix) # LIVIERIS\n",
    "    # Calculate test loss/MSE\n",
    "    valid_loss = np.mean(metrics['losses'])\n",
    "    valid_accuracy = np.mean(metrics['accuracy'])\n",
    "    valid_AUC = np.mean(metrics['AUC'])\n",
    "\n",
    "    # Elapsed time per epoch\n",
    "    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "\n",
    "    # Store performance\n",
    "    history['train_loss'].append(train_loss)    \n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['train_accuracy'].append(train_accuracy)    \n",
    "    history['valid_accuracy'].append(valid_accuracy)   \n",
    "    history['train_AUC'].append(train_AUC)    \n",
    "    history['valid_AUC'].append(valid_AUC)   \n",
    "\n",
    "    # Update best model\n",
    "    if valid_AUC > best_AUC:\n",
    "        print('[INFO] Model saved')\n",
    "        if (not os.path.exists(params['checkpoints_path'])):\n",
    "            os.mkdir(params['checkpoints_path'])\n",
    "        torch.save(model, os.path.join(params['checkpoints_path'], \"model.pth\"))\n",
    "        best_AUC = valid_AUC    \n",
    "        \n",
    "    # Learning rate scheduler\n",
    "    scheduler(valid_AUC)\n",
    "\n",
    "    # Early Stopping\n",
    "    if early_stopping(valid_AUC): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the model\n",
    "# torch.onnx.export(model=model,               # model being run\n",
    "#                   args=(img1, img2),         # model input (or a tuple for multiple inputs)\n",
    "#                   f=\"Siamese_network.onnx\",  # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                   opset_version=10,          # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],   # the model's input names\n",
    "#                   output_names = ['output'], # the model's output names\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(history)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
    "df_results[['train_accuracy','valid_accuracy']].plot(ax=ax[0], marker='o')\n",
    "df_results[['train_loss','valid_loss']].plot(ax=ax[1], marker='o')\n",
    "ax[0].legend(frameon=False, fontsize=12);\n",
    "ax[1].legend(frameon=False, fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized model\n",
    "model = torch.load(params['checkpoints_path'] + '/model.pth')\n",
    "model.eval()\n",
    "\n",
    "L = []\n",
    "ConfusionMatrix = np.array([[0,0], [0,0]]) # LIVIERIS\n",
    "# setup loop with TQDM and dataloader\n",
    "loop = tqdm(test_dl, leave=True)\n",
    "# setup epoch's metrics\n",
    "metrics = {'losses': [], 'accuracy': [], 'AUC': []}\n",
    "for step, (img1, img2, labels) in enumerate(loop):\n",
    "    img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "    # Get loss & predictions\n",
    "    predictions, loss = model(img1, img2, labels)\n",
    "    L += [predictions.detach().cpu().numpy()]\n",
    "    # Calculate performance metrics\n",
    "    accuracy, AUC, CM = performance_evaluation(labels, predictions)               \n",
    "    ConfusionMatrix += CM\n",
    "    # Add loss/accuracy/AUC\n",
    "    metrics['losses'].append(loss.item())\n",
    "    metrics['accuracy'].append(accuracy)\n",
    "    metrics['AUC'].append(AUC)   \n",
    "\n",
    "\n",
    "    # add stuff to progress bar in the end\n",
    "    loop.set_description(\"Testing\")\n",
    "    loop.set_postfix(loss=f\"{np.mean(metrics['losses']):.3f}\",\n",
    "                        accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\",\n",
    "                        AUC=f\"{np.mean(metrics['AUC']):.3f}\")\n",
    "    \n",
    "\n",
    "print(f\"[INFO] Loss: {np.mean(metrics['losses']):.3f}\")\n",
    "print(f\"[INFO] AUC: {np.mean(metrics['AUC']):.3f}\")\n",
    "print(f\"[INFO] Accuracy: {np.mean(metrics['accuracy']):.2f}%\")\n",
    "print(ConfusionMatrix) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [t for x in L for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 520\n",
    "test_dataset[idx], L[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
